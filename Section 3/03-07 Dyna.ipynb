{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "One problem with Q-Learning is that it takes many experience tuples to converge, because you need to take a real step (trade) and wait for the results.\n",
    "\n",
    "Dyna works building models of T and R and for each real interaction, we hallucinate a few 100 to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q Big Picture\n",
    "Q-Learning (expensive)\n",
    "- init Q table\n",
    "- observe s\n",
    "- execute a, observe s, r\n",
    "- update Q with (s,a,s',r)\n",
    "\n",
    "Dyna-Q (cheap, 100's of times)\n",
    "- Learn Models of T and R: Find new values of T and R.\n",
    "    - T'(s,a,s') =\n",
    "    - R'(s,a) = \n",
    "- Hallucinate Experience\n",
    "    - s = random\n",
    "    - a = action\n",
    "    - s' = infer from T\n",
    "    - r = R(s, a)\n",
    "- update Q\n",
    "    - with (s, a, s', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning T - methods from teacher\n",
    "T(s, a, s') prob s,a -> s'\n",
    "\n",
    "init $T_c$() = to be very small numbers = 0.000001\n",
    "\n",
    "while executing, observe (s,a,s')\n",
    "\n",
    "increment $T_c$(s,a,s')\n",
    "\n",
    "$T_c(s,a,s')$: Probability of ending up in state s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$T(s,a,'s) = \\frac{T_c(s,a,s')}{\\sum_i T_c(s,a,i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning R\n",
    "- R(s, a): expected reward from model\n",
    "- r: immediate reward from real world.\n",
    "$$R'(s,A) = (1-\\alpha) \\cdot R(s,a) + \\alpha \\cdot r$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The Dyna architecture consists of a combination of:\n",
    "\n",
    "- direct reinforcement learning from real experience tuples gathered by acting in an environment,\n",
    "- updating an internal model of the environment, and,\n",
    "- using the model to simulate experiences.\n",
    "\n",
    "Dyna learning architecture\n",
    "\n",
    "![dynaq](dynaq.png)\n",
    "\n",
    "Sutton and Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998. [web](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "\n",
    "- Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference on Machine Learning, Austin, TX, 1990. [pdf](https://webdocs.cs.ualberta.ca/~sutton/papers/sutton-90.pdf)\n",
    "- Sutton and Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.[web](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)\n",
    "    \n",
    "- [RL course by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) (videos, slides)\n",
    "    - Lecture 8: Integrating Learning and Planning [pdf]http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/dyna.pdf      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
