{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focused on learners with forecast price changes.  We buy or sell on significant changes.\n",
    "\n",
    "We don't know the certainty and wehn to exit positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RL Problem\n",
    "Linear Regression is a solution the Supervised Regression Problem.\n",
    "\n",
    "For example:\n",
    "- Robot senses, thinks then act.\n",
    "- Reads the Environment.\n",
    "- Thinks p(s)\n",
    "- Acts p(s) = a\n",
    "- Leads to new state T.\n",
    "- T is the new s and the cycle begins anew.\n",
    "- How do we find the policy P?\n",
    "- Every action should come with some reward. R\n",
    "- The policy should take actions to maximize rewards.\n",
    "\n",
    "In terms of finance,\n",
    "- S: Market features, whether we are holding the stock.\n",
    "- P: \n",
    "- A: Buying or selling or do nothing\n",
    "- R: return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision problems\n",
    "- Set of states A\n",
    "- Set of actions A\n",
    "- Transition function T(s,a,s') The probability that we will get to s' from s if we take action a.  We need to know every probability we will hit between the 2.\n",
    "- Reward function R(s, a)\n",
    "\n",
    "Find policy P*(s) that will maximize reward.  The optimal P.  If we have T and R, we can use policy iteration and value iteration to get P.  We do not know T and R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown transitions and rewards\n",
    "We do not have transition or reward functions.\n",
    "\n",
    "Example:\n",
    "- <s1, a1, s1', r1> experience tuple\n",
    "- <s1, a2, s2', r2>\n",
    "...\n",
    "\n",
    "- Model-based build of T(s,a,s') we look to see what happens when we have s and take a and record it.\n",
    "- R: same thing but take the r\n",
    "\n",
    "- Model-free:\n",
    "    Q-learning - They develop a policy by just looking at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to optimize?\n",
    "- Imagine the robot has infinit moves vs finite moves (3 vs 8)\n",
    "- Infinite horizon -> try to maximize rewards to infinity\n",
    "$$\\sum_{i=1}^{\\infty}r_i$$\n",
    "\n",
    "finite horizon\n",
    "$$\\sum_{i=1}^{n}r_i$$\n",
    "discounted reward - used in Q learning\n",
    "$$\\sum_{i=1}^{\\infty}\\lambda^{i-1}r_i$$\n",
    "\n",
    "![optimize](optimize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Summary\n",
    "- RL algos solve Markov Decision Problems\n",
    "- State, Action, Transition, Reward are used to define MDPs\n",
    "- Find P(s)->a to maximize reward\n",
    "- T would be the market in our example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
