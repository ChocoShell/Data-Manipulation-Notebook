{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Q?\n",
    "Q(s,a) = immediate reward + discounted reward\n",
    "- Q does not think short term\n",
    "\n",
    "##### How to use Q?\n",
    "P(s) = argmax_a(Q(s,a)) finds the a that maximizes Q(s)\n",
    "\n",
    "P*(s) = Q*(s,a)\n",
    "\n",
    "How do we build that Q table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Procedure\n",
    "Big Picture\n",
    "- Select training data\n",
    "- iterate over time (s,a,s',r)\n",
    "- test policy P\n",
    "- repeat until converge (until it does't get any better.  No better return)\n",
    "Details\n",
    "- set starttime, init Q: We initialize our Q table with small random numbers.\n",
    "- compute s\n",
    "- select a\n",
    "- observe r, s'\n",
    "- update Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rule\n",
    "Updating Q:\n",
    "- $\\alpha$ learning rate between 0 to 1.0 usually .2\n",
    "- $\\lambda$ discount rate from 0 to 1.0\n",
    "$$Q'(s,A) = (1 - \\alpha)\\cdot Q(s,a) + \\alpha \\cdot improved estimate$$\n",
    "\n",
    "$$Q'(s,A) = (1 - \\alpha)\\cdot Q(s,A) + \\alpha \\cdot (r + \\lambda \\cdot Q(s', argmax_{A'}(Q(s', A'))))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rule Notes\n",
    "\n",
    "The formula for computing Q forany state-action pair (s, a), given an experience tuple (s, a, s', r), is:\n",
    "\n",
    "Q'[s, a] = (1 - α) · Q[s, a] + α · (r + γ · Q[s', argmaxa'(Q[s', a'])])\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "- r = R[s, a] is the immediate reward for taking action a in state s,\n",
    "- γ ∈ [0, 1] (gamma) is the discount factor used to progressively reduce the value of future rewards,\n",
    "- s' is the resulting next state,\n",
    "- argmaxa'(Q[s', a']) is the action that maximizes the Q-value among all possible actions a' from s', and,\n",
    "- α ∈ [0, 1] (alpha) is the learning rate used to vary the weight given to new experiences compared with past Q-values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Finer Points\n",
    "- Success depends on exploration: One way to do this is with randomness.\n",
    "- Choose random action with prob c.\n",
    "\n",
    "Set c to .3 at the beginning of learning and make it smaller and smaller until we have multiple iterations.  It lets us arrive at different states we may not otherwise arrive at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trading Problem: Actions\n",
    "- Buy\n",
    "- Sell\n",
    "- Nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trading Problem: Rewards\n",
    "Which results in faster convergence?\n",
    "- r = daily return. Immediate reward\n",
    "- r = 0 until exit, then cumulative return. Delayed Reward\n",
    "\n",
    "Daily returns.  If we choose the other one, the learner has to infer from all the way back and see if every action is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trading Problem: State\n",
    "What belongs in state?\n",
    "- adjusted close/SMA: adj. close and SMA separately are bad because adj. close and SMA differs between stocks greatly\n",
    "- Bollinger Band Value\n",
    "- P/E ratio\n",
    "- Holding stock\n",
    "- return since entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the state\n",
    "- State is an integer. (Not rounding)\n",
    "- discretize each factor\n",
    "- combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretizing or Discretization\n",
    "We want to turn a real number on a limited scale (e.g., 0 to 9)\n",
    "```\n",
    "stepsize = size(data)/steps\n",
    "data.sort()\n",
    "for i in range(0, steps):\n",
    "    threshold(i) = data((i+1) * stepsize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Recap\n",
    "Building a model\n",
    "- define states, actions, rewards\n",
    "- choose in-sample training period\n",
    "- iterate: Q-table update\n",
    "- backtest\n",
    "- repeat last 2 steps\n",
    "\n",
    "Training a model\n",
    "- backtest on later data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "Advantages\n",
    "\n",
    "- The main advantage of a model-free approach like Q-Learning over model-based techniques is that it can easily be applied to domains where all states and/or transitions are not fully defined.\n",
    "- As a result, we do not need additional data structures to store transitions T(s, a, s') or rewards R(s, a).\n",
    "- Also, the Q-value for any state-action pair takes into account future rewards. Thus, it encodes both the best possible value of a state (maxa Q(s, a)) as well as the best policy in terms of the action that should be taken (argmaxa Q(s, a)).\n",
    "\n",
    "Issues\n",
    "\n",
    "- The biggest challenge is that the reward (e.g. for buying a stock) often comes in the future - representing that properly requires look-ahead and careful weighting.\n",
    "- Another problem is that taking random actions (such as trades) just to learn a good strategy is not really feasible (you'll end up losing a lot of money!).\n",
    "- In the next lesson, we will discuss an algorithm that tries to address this second problem by simulating the effect of actions based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- CS7641 Machine Learning, taught by Charles Isbell and Michael Littman\n",
    "    - Watch for free on [Udacity](https://classroom.udacity.com/courses/ud262) (mini-course 3, lessons RL 1 - 4)\n",
    "    - Watch for free on [YouTube](https://www.youtube.com/watch?v=_ocNerSvh5Y&list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp)\n",
    "    - Or take the course as part of the OMSCS program!\n",
    "- [RL course by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) (videos, slides)\n",
    "- [A Painless Q-Learning Tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
