{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create policies that tell us when to enter and exit.\n",
    "\n",
    "RL is a problem not a solution.  Similar to lin reg is a solution to the supervised regression problem.\n",
    "\n",
    "Here is the problem for the robot.\n",
    "\n",
    "Example:\n",
    "\n",
    "Robot and an environment:\n",
    "\n",
    "Robot observes environment.  S is what we see in the environment. There is also R, a reward we get from taking the action.\n",
    "\n",
    "Robot has a policy based on s that provides an action -> P(S) = A\n",
    "\n",
    "The environment then takes the action and transitions to a new state.\n",
    "\n",
    "The question is, how do we find P?\n",
    "\n",
    "Imagine that the robot accumulates these rewards and tries to tkae actions that maximize these rewards.  There is an algo that takes all this information over time and to find the best P."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put image here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example taken further: Mapping Trading to RL\n",
    "\n",
    "Environment = Market\n",
    "State = Market Features, Prices, Holding or not, etc.\n",
    "Actions = BUY, SELL, NOTHING\n",
    "\n",
    "\n",
    "We can reverse engineer the policy with the rewards and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Problem\n",
    "\n",
    "- Set of States S\n",
    "- Set of actions A\n",
    "- Transition function T(s,a,s') 3 dimensional object. it is the probability that if we are in state S and take action A that we will get to s'.\n",
    "\n",
    "The sum if we are in state S and take action A of the states that we might end up in, we have probability 1 that we will end up in a new state.\n",
    "\n",
    "- Reward function R(s,a) what is the reward if we take action a if we are in s\n",
    "\n",
    "Find policy P*(s) optimal, that will maximize reward.\n",
    "\n",
    "2 types of thigns are policy iteraion and value iteraion, we need T and R though which we do not have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown transitions and rewards\n",
    "\n",
    "(s1, a1, s2, r1) Experience tuple, similar to RL.\n",
    "(s2,a2,s3,r2)\n",
    "\n",
    "Take a new action and get a new state and reward, etc, etc, etc.\n",
    "\n",
    "If we have this trail, we can do 2 things.\n",
    "\n",
    "First set is Model-based appraoch:\n",
    "\n",
    "Build Model of T(s,a,s2) and R(s,a)\n",
    "Then we can use value/policy iteration to get P\n",
    "\n",
    "- Model Free: One type is Q learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show optimize picture\n",
    "\n",
    "Talk about infinite and finite horizon and reduction in rewards.  $\\lambda = .95$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Summary\n",
    "- The problem for RL algos are a MDPs. RL algos solve that.\n",
    "- S, A, T, R Find P\n",
    "- Map to trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Udacity\n",
    "- Supervised Learning: y = f(x)  Find some f that maps x to y\n",
    "- Unsupervised Learning: find some f that gives you a compact description of the x's\n",
    "- Reinforcement Learning: y = f(x) z.  Given pairs x,z.  Find some f that generates y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes\n",
    "Explain state with grid and state may or may not work correctly.\n",
    "\n",
    "Transition Fucntion -> \"Rules of the game\"\n",
    "\n",
    "Markovian property \n",
    "- only present matters.  Transition only depends on one state, s\n",
    "- Things are stationary\n",
    "\n",
    "Reward: Just a scalar value\n",
    "\n",
    "MDP define problems, solution is the policy: P(s) -> a, There is a P* that maximizes long term reward.\n",
    "\n",
    "We have a lot of (s,a,r) pairs.  P is the f, r is z, y is a, s is x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
