{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is reinforcement learning?\n",
    "How software should act in an environment to maximize reward.\n",
    "\n",
    "For example, in trading, we want it to create a policy that tell us when to buy and sell.\n",
    "\n",
    "RL is a problem not a solution.  Similar to lin reg is a solution to the supervised regression problem.\n",
    "\n",
    "Here is the problem for the robot.\n",
    "\n",
    "Example:\n",
    "\n",
    "Robot and an environment:\n",
    "\n",
    "Robot observes environment.  S is what we see in the environment. There is also R, a reward we get from taking the action.\n",
    "\n",
    "Robot has a policy based on s that provides an action -> P(S) = A\n",
    "\n",
    "The environment then takes the action and transitions to a new state and a reward.\n",
    "\n",
    "The question is how do we find P?\n",
    "\n",
    "Imagine that the robot accumulates these rewards and tries to tkae actions that maximize these rewards.  There is an algo that takes all this information over time and to find the best P."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![robot](robot.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process\n",
    "\n",
    "How we model a RL problem.\n",
    "\n",
    "- Set of States S\n",
    "- Set of actions A\n",
    "- Transition function T(s,a,s') 3 dimensional object. it is the probability that if we are in state S and take action A that we will get to s'.\n",
    "\n",
    "The sum if we are in state S and take action A of the states that we might end up in, we have probability 1 that we will end up in a new state.\n",
    "\n",
    "- Reward function R(s,a) what is the reward if we take action a if we are in s\n",
    "\n",
    "Find policy P*(s) optimal, that will maximize reward.\n",
    "\n",
    "2 types of solution methods are policy iteration and value iteration, we need T and R though which we do not have.\n",
    "\n",
    "### Markov Decision Process (Machine Learning Udacity)\n",
    "(Explain state with grid and state may or may not work correctly.)\n",
    "\n",
    "Transition Fucntion -> \"Rules of the game\"\n",
    "\n",
    "Markovian property \n",
    "- only present matters.  Transition only depends on one state, s\n",
    "- Things are stationary\n",
    "\n",
    "Reward: Just a scalar value\n",
    "\n",
    "MDP define problems, solution is the policy: P(s) -> a, There is a P* that maximizes long term reward.\n",
    "\n",
    "We have a lot of (s,a,r) pairs.  P is the f, r is z, y is a, s is x.\n",
    "\n",
    "Instead of seeing (s,a) pairs we see (s,a,r) tuples (s, a) -> r\n",
    "\n",
    "s = x, a = y, r = z, P = f.\n",
    "\n",
    "A policy is just a function that tells you which action to take for a given state.\n",
    "\n",
    "We only care about the best action for just the state we are in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mdp](mdp.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stochasticoptimal](stochasticoptimal.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown transitions and rewards\n",
    "\n",
    "(s1, a1, s2, r1) Experience tuple, similar to SL.\n",
    "(s2,a2,s3,r2)\n",
    "\n",
    "Take a new action and get a new state and reward, etc, etc, etc.\n",
    "\n",
    "If we have this trail, we can do 2 things.\n",
    "\n",
    "First set is Model-based appraoch:\n",
    "\n",
    "Build Model of T(s,a,s2) and R(s,a)\n",
    "Then we can use value/policy iteration to get P\n",
    "\n",
    "- Model Free: One type is Q learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimize](optimize.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![quiz](quiz.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Summary\n",
    "- The problem for RL algos are a MDPs. RL algos solve that.\n",
    "- S, A, T, R Find P\n",
    "- Map to trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![utility](utility.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nonargu](nonargu.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility (following a policy) is the expected states you see following a specific policy. \n",
    "\n",
    "Utility can be seen as current reward and all future rewards if you follow a given policy.\n",
    "\n",
    "Argmax is finding the optimal arg.\n",
    "\n",
    "Gamma is the discount\n",
    "\n",
    "R is the reward function.\n",
    "\n",
    "P is the policy\n",
    "\n",
    "\\* is the optimal\n",
    "\n",
    "E is expectation\n",
    "\n",
    "Last line is Bellman equation. Very Important!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![policyquiz](policyquiz.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![policyquiz2](policyquiz2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rlapi](rlapi.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![threeapproaches](threeapproaches.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. State to Action -> Policy Search.\n",
    "    - Learning Policy is very indirect but direct use.\n",
    "\n",
    "2. Utility maps states to values -> Value-function based.  If we're acting in the world, we can be in a state and take an action to get a value.\n",
    "\n",
    "3. Transition Function, Reward function. -> model-based.  Direct learning but indirect use.  Can be solved as a Supervised Learning problem.\n",
    "\n",
    "Note: value-function based is Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Udacity\n",
    "- Supervised Learning: y = f(x)  Find some f that maps x to y\n",
    "- Unsupervised Learning: find some f that gives you a compact description of the x's\n",
    "- Reinforcement Learning: y = f(x) z.  Given pairs x,z.  Find some f that generates y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes\n",
    "(Explain state with grid and state may or may not work correctly.)\n",
    "\n",
    "Transition Fucntion -> \"Rules of the game\"\n",
    "\n",
    "Markovian property \n",
    "- only present matters.  Transition only depends on one state, s\n",
    "- Things are stationary\n",
    "\n",
    "Reward: Just a scalar value\n",
    "\n",
    "MDP define problems, solution is the policy: P(s) -> a, There is a P* that maximizes long term reward.\n",
    "\n",
    "We have a lot of (s,a,r) pairs.  P is the f, r is z, y is a, s is x.\n",
    "\n",
    "Instead of seeing (s,a) pairs we see (s,a,r) tuples (s, a) -> r\n",
    "\n",
    "s = x, a = y, r = z, P = f.\n",
    "\n",
    "A policy is just a function that tells you which action to take for a given state.\n",
    "\n",
    "We only care about the best action for just the state we are in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP: Rewards\n",
    "\n",
    "- delayed rewards: Chess game(playing well with 1 blunder, playing badly for all turns), Board with only +1 in one square.\n",
    "\n",
    "    - You will have an array of (s,a,r) tuples and chains leading to rewards, we need to figure out where we went wrong. Temporal Credit Assignment\n",
    "\n",
    "- minor changes matter: R(s) = -0.04.  The reward for every state not explicity marked.  What do these small negative rewards encourage you to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUIZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function drastically changes what approach we take."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
