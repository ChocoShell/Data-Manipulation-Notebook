{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qlearningbigpicture](qlearningbigpicture.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qupdate](qupdate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qlearningfinerpoints](qlearningfinerpoints.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Udacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![qquiz](qquiz.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we define U and $\\pi$ in terms of Q?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Qlearning](Qlearning.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vconvergence](vconvergence.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a model free approach, it does not use models of T or R.  We build a table of utility values as we interact with the world.  It is evaluating the Bellman equations for data.\n",
    "\n",
    "Note: we had R and T and now we only have access to transitions.\n",
    "\n",
    "Named after Q function:\n",
    "- Q(s,a): Table.  S = State, A = Action.  Q represents the value you get for taking action A from State S.\n",
    "- immediate reward and discounted reward.\n",
    "\n",
    "How to use Q?\n",
    "- P(s) = argmax_a(Q(s,a)) What value of a maximizes Q at state s.\n",
    "- P*(s) = optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rule\n",
    "Updating Q:\n",
    "- $\\alpha$ learning rate between 0 to 1.0 usually .2\n",
    "- $\\lambda$ discount rate from 0 to 1.0\n",
    "$$Q'(s,A) = (1 - \\alpha)\\cdot Q(s,a) + \\alpha \\cdot improved estimate$$\n",
    "\n",
    "$$Q'(s,A) = (1 - \\alpha)\\cdot Q(s,A) + \\alpha \\cdot (r + \\lambda \\cdot Q(s', argmax_{A'}(Q(s', A'))))$$\n",
    "\n",
    "\n",
    "$Q(s', argmax_{A'}(Q(s', A')))$ = Future discounted rewards.\n",
    "\n",
    "### Two Finer Points\n",
    "- Success depends on exploration: One way to do this is with randomness.\n",
    "- Choose random action with prob c.\n",
    "\n",
    "Set c to .3 at the beginning of learning and make it smaller and smaller until we have multiple iterations.  It lets us arrive at different states we may not otherwise arrive at.\n",
    "\n",
    "Flip a coin: Are we gonna pick a random action? What random action will we take?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
